<h1> Dont-Overfit </h1>

<h2> Like to Problem Statement: </h2>

https://www.kaggle.com/c/dont-overfit-ii/overview

<h2> Link to Datasets: </h2>

https://www.kaggle.com/c/dont-overfit-ii/data

<h2> Experimentation: </h2>

<h3> Selecting a Classification Model: </h3>

Let us try out a few Classification Models without any Dimensionality Reduction and select the best one for future experimentation.

<ol>
<li> Logistic Regression (Accuracy = 0.662) </li>
<li> Support Vector Classifier (Accuracy = 0.663) </li>
<li> Decision Tree Classifier (Accuracy = 0.562) </li>
<li> Gaussian Naive Bayes Classifier (Accuracy = 0.611) </li>
<li> Gaussian Process Classifier (Accuracy = 0.526) </li>
</ol>

<stong> NOTE: </strong> Python Implementation for the above Classification Models can be found in the Classification Models Directory

<h3> Dimensionality Reduction </h3>

Yet to be done.......


