# Dont-Overfit

## Like to Problem Statement:

https://www.kaggle.com/c/dont-overfit-ii/overview

## Link to Datasets:

https://www.kaggle.com/c/dont-overfit-ii/data

## Experimentation

### Selecting a Classification Model

<strong>Let us try out a few Classification Models without any Dimensionality Reduction and select the best one for future experimentation.</strong>

<ol>
<li>Logistic Regression (Accuracy = 0.662)</li>
<li>Support Vector Classifier (Accuracy = 0.663)</li>
<li>Decision Tree Classifier (Accuracy = 0.562)</li>
<li>Gaussian Naive Bayes Classifier (Accuracy = 0.611)</li>
</ol>

<strong>NOTE: Python Implementation for the above can be found in the Classification Models Directory</strong>

### Dimensionality Reduction

Yet to be done.......


